{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어_CH3\n",
    "## 자연어 처리 개요\n",
    "\n",
    "### 텍스트 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갑작스런': 1.4054651081081644, '내일은': 1.4054651081081644, '놀러왔다가': 1.4054651081081644, '망연자실하고': 1.4054651081081644, '반가운': 1.4054651081081644, '비로': 1.4054651081081644, '서쪽을': 1.4054651081081644, '소식이': 1.4054651081081644, '오늘도': 1.4054651081081644, '이어졌는데요': 1.4054651081081644, '인해': 1.4054651081081644, '있습니다': 1.0, '중심으로': 1.4054651081081644, '폭염을': 1.4054651081081644, '폭염이': 1.4054651081081644, '피해서': 1.4054651081081644, '휴일에': 1.4054651081081644, '휴일인': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sent = ('휴일인 오늘도 서쪽을 중심으로 폭염이 이어졌는데요, 내일은 반가운 비 소식이 있습니다.', \n",
    "        '폭염을 피해서 휴일에 놀러왔다가 갑작스런 비로 인해 망연자실하고 있습니다.')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sent) # 문장 벡터화 진행\n",
    "\n",
    "idf = tfidf_vectorizer.idf_\n",
    "print(dict(zip(tfidf_vectorizer.get_feature_names(), idf))) # 각 수치에 대한 값 시각화\n",
    "\n",
    "# TF-IDF로 벡터화한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자카드 유사도\n",
    "- 두 문장을 각각 단어의 집합으로 만든 뒤 두 집합을 통해 유사도를 측정하는 방식 중 하나\n",
    "- 유사도를 측정하는 방법은 두 집합의 교집합인 공통된 단어의 개수를 두 집합의 합집합, 즉 전체 단어의 수로 나누면 된다.\n",
    "- 1에 가까울수록 유사도가 높다는 의미\n",
    "\n",
    "### 코사인 유사도\n",
    "- 코사인 유사도는 두 개의 벡터값에서 코사인 각도를 구하는 방법\n",
    "- 값은 -1과 1 사이의 값을 가지고 1에 가까울수록 유사하다는 것을 의미\n",
    "- 두 벡터 간의 각도를 구하는 것이기 때문에 방향성의 개념이 더해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05629716]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코사인 유사도\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]) # 첫 번째와 두 번째 문장 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유클리디언 유사도\n",
    "- 가장 기본적인 거리를 측정하는 유사도 공식\n",
    "- 유클리디언 거리, L2 거리 라고 불린다.\n",
    "- n 차원 공간에서 두 점 사이의 최단 거리를 구하는 접근법\n",
    "- 1 이상의 값을 가질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.37382884]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "euclidean_distances(tfidf_matrix[0:1], tfidf_matrix[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22387021]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 정규화 방법 (L1 - Normalization)\n",
    "# 각 벡터 안의 요소 값을 모두 더한 것이 크기가 1이 되도록 벡터들의 크기를 조절하는 방법\n",
    "# 벡터의 모든 값을 더한 후 이 값으로 각 벡터의 값을 나누면 된다.\n",
    "\"\"\"\n",
    "제한이 없는 유사도 값은 사용하기 어렵기 때문에 값을 제한해야 한다. (0 과 1 사이의 값을 갖도록 만든다.)\n",
    "앞서 각 문장을 벡터화했었는데, 이 벡터를 일반화(Normalize)한 후 다시 유클리디언 유사도를 측정하면 0과 1 사이의 값을 갖게 된다.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def l1_normalize(v):\n",
    "    norm = np.sum(v)\n",
    "    return v/norm\n",
    "\n",
    "tfidf_norm_l1 = l1_normalize(tfidf_matrix)\n",
    "euclidean_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 맨하탄 유사도\n",
    "- 맨하탄 거리를 통해 유사도를 측정하는 방법\n",
    "- 맨하탄 거리란 사각형 격자로 이뤄진 지도에서 출발점에서 도착점까지를 가로지르지 않고 갈 수 있는 최단거리를 구하는 공식\n",
    "- 맨하탄은 L1 거리라고 부른다\n",
    "- 값이 계속 커질 수 있기 때문에 L1 정규화 방법을 사용해 벡터 안의 요소 값을 정규화한 뒤 유사도를 측정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92479112]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "manhattan_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "data_set = tf.keras.utils.get_file(fname = 'imdb.tar.gz',\\\n",
    "                                  origin='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\\\n",
    "                                  extract = True) # 압축 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축이 풀린 데이터들이 directory 안에 txt 파일 형태로 있어서 판다스의 데이터프레임을 만들기 위해서는 변환 작업을 진행해야 한다.\n",
    "\n",
    "# 각 파일에서 리뷰 텍스트를 불러오는 함수\n",
    "def directory_data(directory): # 데이터를 가져올 Directory를 인자로 받는다.\n",
    "    data = {}\n",
    "    data['review'] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file_path), \"r\", encoding='UTF-8')as file:\n",
    "            data[\"review\"].append(file.read())\n",
    "            \n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "\n",
    "# 각 리뷰에 해당하는 라벨값을 가져오는 함수\n",
    "def data(directory):\n",
    "    pos_df = directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df['sentiment'] = 1\n",
    "    neg_df['sentiment'] = 0\n",
    "    \n",
    "    return pd.concat([pos_df, neg_df])\n",
    "\n",
    "\n",
    "# 앞에서 설명한 두 함수를 호출해서 판다스 데이터프레임을 반환받는 구문\n",
    "train_df = data(os.path.join(os.path.dirname(data_set), \"aclImdb\", \"train\"))\n",
    "test_df = data(os.path.join(os.path.dirname(data_set), \"aclImdb\", \"test\"))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reviews = list(train_df['review'])\n",
    "\n",
    "# reviews 에는 각 문장을 리스트로 담고 있다. \n",
    "# 단어를 토크나이징하고 문장마다 토크나이징된 단어의 수를 저장하고,\n",
    "# 그 단어들을 붙여 알파벳의 전체 개수를 저장하는 부분을 만들어 보자\n",
    "\n",
    "# 문자열 문장 리스트를 토크나이징\n",
    "tokenized_reviews = [r.split() for r in reviews]\n",
    "\n",
    "\n",
    "# 토크나이징된 리스트에 대한 각 길이를 저장\n",
    "review_len_by_token = [len(t) for t in tokenized_reviews]\n",
    "\n",
    "\n",
    "# 토크나이징된 것을 붙여서 음절의 길이를 저장\n",
    "review_len_by_eumjeol = [len(s.replace(' ','')) for s in reviews]\n",
    "\n",
    "\"\"\"\n",
    "위와 같이 만드는 이유는 문장에 포함된 단어와 알파벳의 개수에 대한 데이터 분석을 수월하게 만들기 위해서이다.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 히스토그램으로 문장을 구성하는 단어의 개수와 알파벳 개수를 알아보자\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그래프에 대한 이미지 크기 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "# 히스토그램 선언\n",
    "# bins : 히스토그램 값에 대한 버킷 범위\n",
    "# alpha : 그래프 색상 투명도\n",
    "# color : 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(review_len_by_token, bins = 50, alpha = 0.5, color='r', label='word')\n",
    "plt.hist(review_len_by_eumjeol, bins = 50, alpha = 0.5, color = 'b', label = 'alphabet')\n",
    "plt.yscale('log', nonposy = 'clip')\n",
    "\n",
    "# 그래프 제목\n",
    "plt.title('Review Length Histogram')\n",
    "\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Review Length')\n",
    "\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Number of Reviews')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에 대한 길이 분포\n",
    "# 빨간색 히스토그램은 단어 개수에 대한 히스토그램이고, 파란색은 알파벳 개수의 히스토그램이다.\\\n",
    "\n",
    "# 데이터 분포를 통계치로 수치화\n",
    "import numpy as np\n",
    "\n",
    "print('문장 최대 길이: {}'.format(np.max(review_len_by_token)))\n",
    "print('문장 최소 길이: {}'.format(np.min(review_len_by_token)))\n",
    "print('문장 평균 길이: {:.2f}'.format(np.mean(review_len_by_token)))\n",
    "print('문장 길이 표준편차: {:.2f}'.format(np.std(review_len_by_token)))\n",
    "print('문장 중간 길이: {}'.format(np.median(review_len_by_token)))\n",
    "\n",
    "# 사분위의 대한 경우는 0~100 스케일로 돼 있음\n",
    "print('제1사분위 길이: {}'.format(np.percentile(review_len_by_token, 25)))\n",
    "print('제3사분위 길이: {}'.format(np.percentile(review_len_by_token, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "# 박스 플롯 생성\n",
    "# 첫 번째 인자: 여러 분포에 대한 데이터 리스트 입력\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 마크함\n",
    "\n",
    "plt.boxplot([review_len_by_token],\n",
    "             labels = ['token'],\n",
    "             showmeans = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = 'black', width = 800,\n",
    "                     height = 600).generate(' '.join(train_df['review']))\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 데이터에 포함된 단어의 등장 횟수에 따라 단어의 크기가 커지는데 'br'이 엄청 크다\n",
    "# <br> 과 같은 HTML 태그가 포함돼 있기 때문인데, 제거해야 한다 이는 4장의 전처리 단계에서 배운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment = train_df['sentiment'].value_counts()\n",
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(6, 3)\n",
    "sns.countplot(train_df['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
