{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH2 : 자연어 처리 개발 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 01.Tensorflow\n",
    "\n",
    "#### 기존의 Tensorflow 사용\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([5 , 10], -1.0, 1.0))  \n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y = tf.matmul(W, x)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] tf.keras.layers\n",
    "\n",
    "_P.17_\n",
    "### (1) tf.keras.layers.Dense 사용\n",
    "_P.18_\n",
    "\n",
    "dense = tf.keras.layers.Dense( ... )\n",
    "\n",
    "\n",
    "##### 1. 객체 생성 후 다시 호출하면서 입력값 설정\n",
    "dense = tf.keras.layers.Dense( ... )\n",
    "output = dense(input)\n",
    "\n",
    "\n",
    "##### 2. 객체 생성 시 입력 값 설정\n",
    "output = tf.keras.layers.Dense( ... )(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값에 대한 활성화 함수로 시그모이드 함수를 사용\n",
    "# 출력 값으로 10개의 값을 출력하는 완전 연결 계층(Fully Connected Layer)\n",
    "\n",
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(input)\n",
    "# 10개의 노드를 가지는 은닉층이 있고 최종 출력 값은 2개의 노드가 있는 신경망 구조를 생각해보면,\n",
    "# output -> hidden\n",
    "\n",
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(input)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) tf.keras.layers.Dropout\n",
    "_P.21_\n",
    "\n",
    "신경망 모델을 만들 때 생기는 문제점들 중 대표적인 문제점은 과적합(overfitting)이다.\n",
    "과적합 문제는 정규화(Regularization) 방법을 사용해서 해결하는데, 그중 가장 대표적인 방법이 드롭아웃(Dropout)이다.\n",
    "학습 데이터에 과적합되는 상황을 방지하기 위해 학습시 _특정 확률로 노드들의 값을 0으로_ 만든다.\n",
    "\n",
    "\n",
    "tf.keras.layers.Dropout( ... )\n",
    "\n",
    "##### 1. 객체 생성 후 다시 호출하면서 입력값 설정\n",
    "dropout = tf.keras.layers.Dropout( ... )\n",
    "output = dropout(input)\n",
    "\n",
    "##### 2. 객체 생성 시 입력 값 설정\n",
    "output = tf.keras.layers.Dropout( ... )(input)\n",
    "\n",
    "- 학습할 때만 적용되고 예측 혹은 테스트할 때는 적용되지 않아야 한다.\n",
    "- 케라스의 Dropout을 사용할 경우 이러한 부분이 자동으로 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.5)(input)\n",
    "\n",
    "==========================================================================\n",
    "\n",
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(input)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(dropout)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) tf.keras.layers.Conv1D \n",
    "_P.23_\n",
    "\n",
    "\n",
    "합성곱(Convolution) 연산 중 Conv1D\n",
    "자연어 처리 분야에서 사용하는 합성곱의 경우 각 단어(혹은 문자) 벡터의 차원 전체에 대해 필터를  \n",
    "적용시키기 위해 주로 Conv1D를 사용한다.\n",
    "\n",
    "##### 1. 객체 생성 후 다시 호출하면서 입력값 설정\n",
    "conv1d = tf.keras.layers.Conv1D( ... )\n",
    "output = conv1d(input)\n",
    "\n",
    "##### 2. 객체 생성 시 입력 값 설정\n",
    "output = tf.keras.layer.Conv1D( ... )(input)\n",
    "\n",
    "- 인자로 필터의 크기, 필터의 개수, 스트라이드 값 등을 객체를 생성할 때 인자로 설정할 수 있다.\n",
    "- 필터의 가로에 적용되는 kernel_size만 설정하면 된다.  \n",
    "  (Conv1D의 필터는 입력값의 차원수와 높이가 동일하게 연산되기 때문)\n",
    "- 총 몇개의 필터를 사용할지를 filters 인자를 통해서 정해야 한다.\n",
    "- 입력값과 출력값의 가로 크기를 똑같이 만들고 싶다면 padding = 'same'을 지정한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "conv = tf.keras.layers.conv1D(\n",
    "                filters = 10,\n",
    "                kernel_size = 3,\n",
    "                padding = 'same'\n",
    "                activation = tf.nn.relu)(input)\n",
    "                \n",
    "\n",
    "===========================================================================\n",
    "# Dropout 적용한 합성곱 신경망\n",
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(input)\n",
    "conv = tf.keras.layers.conv1D(\n",
    "                filters = 10,\n",
    "                kernel_size = 3,\n",
    "                padding = 'same'\n",
    "                activation = tf.nn.relu)(dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) tf.keras.layers.MaxPool1D\n",
    "_P.28_\n",
    "\n",
    "합성곱 신경망과 함께 쓰이는 기법, 보통 피처 맵(Feature map)의 크기를 줄이거나 주요한 특징을  \n",
    "뽑아내기 위해 합성곱 이후에 적용되는 기법이다.\n",
    "\n",
    "맥스 풀링은(max-pooling) 피처 맵에 대해 최댓값만을 뽑아내는 방식\n",
    "\n",
    "##### 1. 객체 생성 후 다시 호출하면서 입력값 설정\n",
    "max_pool = tf.keras.layers.MaxPool1D( ... )\n",
    "max_pool.apply(input)\n",
    "\n",
    "##### 2. 객체 생성 시 입력 값 설정\n",
    "max_pool = tf.keras.layer.MaxPool1D( ... )(input)\n",
    "\n",
    "\n",
    "1. 입력값이 합성곱과 맥스 풀링을 사용한 후 완전 연결 계층을 통해 최종 출력 값이 나오는 구조\n",
    "1. 입력 ㄱ밧에는 드롭아웃을 적용\n",
    "1. 맥스 풀링 결괏값을 완전 연결 계층으로 연결하기 위해서는 행렬이었던 것을 벡터로 만들어야한다.  \n",
    "   이때 tf.keras.layers.Flatten을 사용한다. (인자값 설정 없이도 사용 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(input)\n",
    "conv = tf.keras.layers.conv1D(\n",
    "                filters = 10,\n",
    "                kernel_size = 3,\n",
    "                padding = 'same'\n",
    "                activation = tf.nn.relu)(dropout)\n",
    "max_pool = tf.keras.layers.MaxPool1D(pool_size = 3, padding = 'same')(conv)\n",
    "flatten = tf.keras.layers.Flatten()(max_pool)\n",
    "hidden = tf.keras.layers.Dense(units = 50, activation = tf.nn.relu)(flatten)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.softmax)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치화된 텍스트 데이터:\n",
      " [[4, 1, 5, 6], [7, 1, 8, 9], [10, 2, 3, 11], [12, 2, 3, 13], [14, 1, 15, 16], [17, 18, 19, 20]]\n",
      "각 단어의 인덱스:\n",
      " {'오늘': 1, '좋은': 2, '일이': 3, '너': 4, '이뻐': 5, '보인다': 6, '나는': 7, '기분이': 8, '더러워': 9, '끝내주는데': 10, '있나봐': 11, '나': 12, '생겼어': 13, '아': 14, '진짜': 15, '짜증나': 16, '환상적인데': 17, '정말': 18, '좋은거': 19, '같아': 20}\n",
      "라벨:  [[1], [0], [1], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "# 전처리 과정에 사용할 모듈 : preprocessing 모듈\n",
    "\n",
    "\n",
    "samples = [\n",
    "    '너 오늘 이뻐 보인다',\n",
    "    '나는 오늘 기분이 더러워',\n",
    "    '끝내주는데, 좋은 일이 있나봐',\n",
    "    '나 좋은 일이 생겼어',\n",
    "    '아 오늘 진짜 짜증나',\n",
    "    '환상적인데, 정말 좋은거 같아'\n",
    "          ]\n",
    "\n",
    "label = [[1], [0], [1], [1], [0], [1]]\n",
    "# label 은 임의로 정한 것으로 특별한 의미는 없다.\n",
    "\n",
    "# 전처리 과정 CH4, CH5, CH6에 자세한 내용이 있다.\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "# 텍스트로 구성된 데이터에서 각 단어를 단어의 인덱스로 바꿨다.\n",
    "# 즉, 텍스트 데이터를 수치화한 것\n",
    "\n",
    "print('수치화된 텍스트 데이터:\\n', sequences)\n",
    "print('각 단어의 인덱스:\\n', word_index)\n",
    "print('라벨: ',label)\n",
    "\n",
    "# 수치화된 텍스트 데이터:\n",
    "#  [[4, 1, 5, 6], '너 오늘 이뻐 보인다'\n",
    "#   [7, 1, 8, 9],\n",
    "#   [10, 2, 3, 11],\n",
    "#   [12, 2, 3, 13],\n",
    "#   [14, 1, 15, 16],\n",
    "#   [17, 18, 19, 20]]\n",
    "\n",
    "# 각 단어의 인덱스:\n",
    "#  {'오늘': 1, '좋은': 2, '일이': 3, '너': 4, '이뻐': 5, '보인다': 6,  \n",
    "#   '나는': 7, '기분이': 8, '더러워': 9, '끝내주는데': 10, '있나봐': 11,\n",
    "#   '나': 12, '생겼어': 13, '아': 14, '진짜': 15, '짜증나': 16,\n",
    "#   '환상적인데': 17, '정말': 18, '좋은거': 19, '같아': 20}\n",
    "\n",
    "# 라벨:  [[1], [0], [1], [1], [0], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0927 13:44:49.551145  2884 deprecation.py:323] From <ipython-input-5-ed9701b5a1e9>:6: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4, 1, 5, 6]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# tf.data를 활용\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "# sequences와 label을 묶어서 조각으로 만들고 함께 사용할 수 있게 해준다.\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "# 데이터를 하나씩 사용할 수 있게 만들어준다.\n",
    "\n",
    "next_data = iterator.get_next()\n",
    "# 데이터가 하나씩 나오게 되는 구조\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "            # try/except 코드를 작성한 이유는 더 이상 불러올 데이터가 없는 경우 범위를 넘어갔다는 에러가 출력되는데\n",
    "            # 이를 막기 위해서 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[4, 1, 5, 6],\n",
      "       [7, 1, 8, 9]]), array([[1],\n",
      "       [0]]))\n",
      "(array([[10,  2,  3, 11],\n",
      "       [12,  2,  3, 13]]), array([[1],\n",
      "       [1]]))\n",
      "(array([[14,  1, 15, 16],\n",
      "       [17, 18, 19, 20]]), array([[0],\n",
      "       [1]]))\n"
     ]
    }
   ],
   "source": [
    "# 배치 크기를 정의한 후 데이터가 배치크기 만큼 한번에 사용할 수 있도록 해본다.\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "            \n",
    "# (array([[4, 1, 5, 6],\n",
    "#        [7, 1, 8, 9]]), array([[1],\n",
    "#        [0]]))\n",
    "# (array([[10,  2,  3, 11],\n",
    "#        [12,  2,  3, 13]]), array([[1],\n",
    "#        [1]]))\n",
    "# (array([[14,  1, 15, 16],\n",
    "#        [17, 18, 19, 20]]), array([[0],\n",
    "#        [1]]))\n",
    "\n",
    "# 한번에 2개의 데이터가 나오는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4, 1, 5, 6]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 셔플하는 과정, 즉 데이터가 섞여서 출력되도록 만든다.\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataset = dataset.shuffle(len(sequences))\n",
    "# 셔플 함수를 적용할 때 인자 값으로는 데이터의 전체 길이를 넣으면 된다.\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4, 1, 5, 6]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n",
      "(array([4, 1, 5, 6]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터를 몇 번 사용하는지를 지칭하는 말이 EPOCH 이다.\n",
    "# 데이터가 여러 번 반복하도록 설정하면 정의한 만큼 데이터를 계속 불러올 수 있다.\n",
    "\n",
    "EPOCH = 2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataset = dataset.repeat(EPOCH)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'x': array([4, 1, 5, 6])}, array([1]))\n",
      "({'x': array([7, 1, 8, 9])}, array([0]))\n",
      "({'x': array([10,  2,  3, 11])}, array([1]))\n",
      "({'x': array([12,  2,  3, 13])}, array([1]))\n",
      "({'x': array([14,  1, 15, 16])}, array([0]))\n",
      "({'x': array([17, 18, 19, 20])}, array([1]))\n"
     ]
    }
   ],
   "source": [
    "# tf.data의 mapping 기능 : 모델에 따라 입력값이 하나가 아니라 두 개 이상이 될 수도 있는데,\n",
    "#                          이때 라벨을 제외한 나머지 데이터를 하나의 입력값으로 묶기 위해 이 과정을 거친다.\n",
    "\n",
    "def mapping_fn(X, Y=None):\n",
    "    input = {'x':X}\n",
    "    label = Y\n",
    "    return input, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataset = dataset.map(mapping_fn)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서는 하나의 입력값만 사용했기 때문에 딕셔너리 안에 하나의 값만 존재한다.\n",
    "# 두 개 이상의 입력값이 존재한다면 다음과 같이 매핑 함수를 정의해야 할 것이다.\n",
    "\n",
    "def mapping_fn(X1, X2, Y=None):\n",
    "    input = {'x1':X1, 'x2':X2}\n",
    "    label = Y\n",
    "    return input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'x': array([[17, 18, 19, 20],\n",
      "       [14,  1, 15, 16]])}, array([[1],\n",
      "       [0]]))\n",
      "({'x': array([[ 4,  1,  5,  6],\n",
      "       [10,  2,  3, 11]])}, array([[1],\n",
      "       [1]]))\n",
      "({'x': array([[ 7,  1,  8,  9],\n",
      "       [12,  2,  3, 13]])}, array([[0],\n",
      "       [1]]))\n",
      "({'x': array([[4, 1, 5, 6],\n",
      "       [7, 1, 8, 9]])}, array([[1],\n",
      "       [0]]))\n",
      "({'x': array([[17, 18, 19, 20],\n",
      "       [12,  2,  3, 13]])}, array([[1],\n",
      "       [1]]))\n",
      "({'x': array([[14,  1, 15, 16],\n",
      "       [10,  2,  3, 11]])}, array([[0],\n",
      "       [1]]))\n"
     ]
    }
   ],
   "source": [
    "# 배치, 셔플, 반복, 매핑 과정을 하나로 섞어서 사용하는 과정\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCH = 2\n",
    "\n",
    "def mapping_fn(X, Y=None):\n",
    "    input = {'x':X}\n",
    "    label = Y\n",
    "    return input, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataset = dataset.map(mapping_fn)\n",
    "dataset = dataset.shuffle(len(sequences))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.repeat(EPOCH)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "# 아래에서부터는 에스티메이터의 기능을 통해 현재 tf.data와 모델이 데이터 파이프라인의 구조로 어떻게 연동되는지 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Estimator\n",
    "\n",
    "고수준 API로 모델 구현에만 집중할 수 있는 환경을 제공한다. 모델 구현 외의 학습(Train), 검증(Evaluate), 예측(Predict)에  \n",
    "필요한 부가적인 구현들은 Estimator의 함수를 사용함으로써 손쉽게 사용할 수 있다. 검증과 평가가 끝난 모델을 사용하기 위한  \n",
    "모델 배포(Export) 역시 에스티메이터를 사용함으로써 간단하게 구현할 수 있다.\n",
    "\n",
    "에스티메이터를 구현하기 위해서는 기본적으로 두 가지 함수를 구현해야 한다. 사용할 모델을 구현한 모델 함수와 모델에 적용될  \n",
    "데이터를 에스티메이터에 전달하기 위한 데이터 입력 함수를 구현해야 한다. 에스티메이터를 통해 올바르게 동작하기 위해서는 정해진 형식을 맞춰서 구현해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params, config):\n",
    "    \n",
    "    # 모델 구현 부분\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec( ... )\n",
    "\n",
    "# features(필수) : 모델에 적용되는 입력값을 의미한다. 하나의 텐서 자료형(tf.Tensor) 혹은 딕셔너리 자료형이어야 한다.\n",
    "# labels(필수) : 모델의 정답 라벨값을 의미한다. 텐서 자료형 or 딕셔너리 자료형\n",
    "# mode : 현재 모델 함수가 실핼된 모드를 의미\n",
    "# params : 모델에 적용될 부가적인 하이퍼파라미터 값을 의미. 딕셔너리 자료형이어야 한다.\n",
    "# config : 모델에 적용할 설정값을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir = ...,\n",
    "                                  configs = ..., params = ...)\n",
    "# 모델 함수(model_fn(필수))와 모델의 변수값 등이 저장되는 체크포인트 파일이 저장될 경로,\n",
    "# 모델의 설정값인 configs와 하이퍼파라미터인 params를 넣어주면 된다.\n",
    "\n",
    "# 모델 학습\n",
    "estimator.train(input_fn = ...)\n",
    "\n",
    "# 학습한 모델 검증\n",
    "estimator.evaluate(input_fn = ...)\n",
    "\n",
    "# 학습한 모델을 통한 예측\n",
    "estimator.predict(input_fn = ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    \n",
    "    # 데이터 파이프라인 구현 부분\n",
    "    \"\"\"파이프라인: 프로세서로 가는 명령어들의 움직임, 또는 명령어를 수행하기 위해\n",
    "                   프로세서에 의해 취해진 산술적인 단계를 연속적이고, 다소 겹치게 만든 것\n",
    "       ex) 1. 파이프라인이 없는 경우: 프로세서가 메모리로부터 첫번째 명령어를 가져와 연산을 수행\n",
    "                                      첫번째 명령어가 끝난 후 두번째 명령어를 가져와 연산을 수행\n",
    "                                      즉, 두번째 명령어가 도착하기 전까지 프로세서는 기다리며 쉬어야한다.\n",
    "           2. 파이프라인이 있는 경우: 프로세서가 메모리로 첫번째 명령어를 가져와 연산을 수행하는 동안\n",
    "                                      두번째 명령어를 가져와 프로세서 근처의 버퍼에 저장해둔다.\"\"\"\n",
    "    # 이 부분에는 데이터의 셔플, 배치, 반복 등의 기능들이 들어갈 것이다.\n",
    "    # 그리고 입력 함수의 경우 보통 학습, 검증, 예측 상황에 맞는 기능들이 모두 다르기 때문에 각 입력 함수를 따로 구현한다.\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 에스티메이터를 활용한 심층 신경망 분류기 구현\n",
    "\n",
    "구현할 모델은 심층 신경망(Deep Neural Network) 구조를 사용해서 앞서 텍스트의 긍/부정을 예측하는 감정 분석(Sentiment Analysis)모델이다.\n",
    "\n",
    "우선 각 단어로 구성된 입력값은 임베딩된 벡터로 변형된다.  \n",
    "이후 각 벡터를 평균해서 하나의 벡터로 만들어준다.  \n",
    "이후에 하나의 은닉층을 거친 후 하나의 결괏값을 뽑는 구조다.  \n",
    "마지막으로 나온 결괏값에 시그모이드 함수를 적용해 0과 1 사이의 값을 구한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 과정\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "samples = [\n",
    "    '너 오늘 이뻐 보인다',\n",
    "    '나는 오늘 기분이 더러워',\n",
    "    '끝내주는데, 좋은 일이 있나봐',\n",
    "    '나 좋은 일이 생겼어',\n",
    "    '아 오늘 진짜 짜증나',\n",
    "    '환상적인데, 정말 좋은거 같아'\n",
    "          ]\n",
    "\n",
    "labels = [[1], [0], [1], [1], [0], [1]]\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data를 활용해서 데이터 입력 함수를 정의\n",
    "EPOCH = 100\n",
    "\n",
    "\n",
    "def train_input_fn():\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    dataset = dataset.repeat(EPOCH)\n",
    "    dataset = dataset.batch(1)\n",
    "    dataset = dataset.shuffle(len(sequences))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "# tf.data의 반복과 셔플 기능을 사용\n",
    "# 정의한 에폭 크기만큼 데이터를 반복시켜서 학습하게 하고,\n",
    "# 셔플 기능을 추가해 모델이 학습을 잘 할 수 있게 한다.\n",
    "# 데이터 입력 함수의 반환값은 이터레이터의 get_next 함수를 사용해서 입력값과 라벨값을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 함수 구현\n",
    "\n",
    "\n",
    "# 우선 모델 함수에서 사용될 상숫값을 정의해준 후 모델 함수를 구현한다.\n",
    "# 정의한 상숫값은 전체 단어 개수와 임베딩 벡터의 크기를 의미한다.\n",
    "VOCAB_SIZE = len(word_index) +1\n",
    "EMB_SIZE = 128\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # 다음으로 모델 함수를 정의하는데 인자로 입력값과 라벨, 그리고 현재 모델 함수가 실행되는 상태인 모드값을 받는다.\n",
    "    # 그리고 현재 실행된 모드가 어떤 상태인지 확인하기 위한 상숫값인 TRAIN, EVAL, PREDICT를 정의한다. \n",
    "    # 해당하는 상태의 상숫값이 True로 설정될 것이다.\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    # 입력값을 임베딩 벡터 형태로 만들어준 후 텍서플로의 reduce_mean 함수를 통해 평균을 구해서 하나의 입력 벡터로 만들어 준다.\n",
    "    embed_input = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features)\n",
    "    embed_input = tf.reduce_mean(embed_input, axis = 1)\n",
    "    \n",
    "    # Dense를 이용해 이 입력값으로 은닉층을 거쳐 출력값을 만든다.\n",
    "    hidden_layer = tf.keras.layers.Dense(128, activation=tf.nn.relu)(embed_input)\n",
    "    output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "    # 최종적으로 나온 출력값에 시그모이드 함수를 적용해 0과 1 사이의 값으로 만들면 모델의 최종 출력값이 나온다.\n",
    "    output = tf.nn.sigmoid(output_layer)\n",
    "    \n",
    "    \n",
    "    # 학습 상태일 때 모델을 학습할 수 있도록 손실(loss) 값과 옵티마이저(optimizer)를 설정해야 한다.\n",
    "    # 손실 함수는 평균 제곱 오차(Mean squared error)를 이용하고\n",
    "    loss = tf.losses.mean_squared_error(output, labels)\n",
    "    \n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        # 옵티마이저의 경우 아담(Adam) 옵티마이저를 사용한다\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "        \n",
    "        # 이렇게 구한 두 값을 학습모드인 경우 EstimatorSpec의 인자값으로 반환하면 모델 함수 구현이 끝난다.\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                train_op=train_op,\n",
    "                loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator를 적용해 실제 학습\n",
    "\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir = DATA_OUT_PATH + 'checkpoint/dnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0927 15:48:15.793053  4424 deprecation.py:323] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0927 15:48:15.831948  4424 deprecation.py:323] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x13d686a41d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
